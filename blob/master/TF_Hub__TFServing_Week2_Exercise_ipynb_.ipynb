{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Копия блокнота \"TFServing_Week2_Exercise.ipynb\"",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX4Kg8DUTKWO"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALCxPdSdX3NU"
      },
      "source": [
        "# Экспорт классификатора MNIST в формате SavedModel\n",
        "В этом упражнении мы узнаем, как создавать модели для TensorFlow Hub. Вам будет поручено выполнить следующие задачи:\n",
        "\n",
        "* Создание простого классификатора MNIST и оценка его точности.\n",
        "* Экспорт в SavedModel.\n",
        "* Размещение модели в качестве модуля TF Hub.\n",
        "* Импорт этого модуля TF Hub для использования с Keras Layers.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbZbp9ZRZ5wV"
      },
      "source": [
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except:\n",
        "    pass"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swaA66rjiRTd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fce8385-afd1-469b-8553-d31a7d90f24b"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "tfds.disable_progress_bar()\n",
        "\n",
        "print(\"\\u2022 Используется TensorFlow версия:\", tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "• Используется TensorFlow версия: 2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMZdLgyN7gby"
      },
      "source": [
        "## Создание классификатора MNIST\n",
        "\n",
        "Мы начнем с создания класса под названием `MNIST`. Этот класс загрузит набор данных MNIST, предварительно обработает изображения из набора данных и построит классификатор на основе CNN. Этот класс также будет иметь несколько методов для обучения, тестирования и сохранения нашей модели.\n",
        "\n",
        "В ячейке ниже введите недостающий код и создайте следующую модель Keras `Sequential`:\n",
        "\n",
        "```\n",
        "    Model: \"sequential\"\n",
        "    _________________________________________________________________\n",
        "    Layer (type)                 Output Shape              Param #   \n",
        "    =================================================================\n",
        "    lambda (Lambda)              (None, 28, 28, 1)         0         \n",
        "    _________________________________________________________________\n",
        "    conv2d (Conv2D)              (None, 28, 28, 8)         80        \n",
        "    _________________________________________________________________\n",
        "    max_pooling2d (MaxPooling2D) (None, 14, 14, 8)         0         \n",
        "    _________________________________________________________________\n",
        "    conv2d_1 (Conv2D)            (None, 14, 14, 16)        1168      \n",
        "    _________________________________________________________________\n",
        "    max_pooling2d_1 (MaxPooling2 (None, 7, 7, 16)          0         \n",
        "    _________________________________________________________________\n",
        "    conv2d_2 (Conv2D)            (None, 7, 7, 32)          4640      \n",
        "    _________________________________________________________________\n",
        "    flatten (Flatten)            (None, 1568)              0         \n",
        "    _________________________________________________________________\n",
        "    dense (Dense)                (None, 128)               200832    \n",
        "    _________________________________________________________________\n",
        "    dense_1 (Dense)              (None, 10)                1290      \n",
        "    =================================================================\n",
        "\n",
        "```\n",
        "\n",
        "Обратите внимание, что мы используем слой `tf.keras.layers.Lambda` в начале нашей модели. Слои `Lambda` используются для обертывания произвольных выражений в виде объекта Layer:\n",
        "\n",
        "```python\n",
        "tf.keras.layers.Lambda(expression)\n",
        "```\n",
        "\n",
        "The `Lambda` layer exists so that arbitrary TensorFlow functions can be used when constructing `Sequential` and Functional API models. `Lambda` layers are best suited for simple operations. \n",
        "Слой `Lambda` существует для того, чтобы произвольные функции `TensorFlow` можно было использовать при построении моделей `Sequential` и  API функций. Слои `Lambda` лучше всего подходят для простых операций."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mpi4NwcZ5wX"
      },
      "source": [
        "class MNIST:\n",
        "    def __init__(self, export_path, buffer_size=1000, batch_size=32,\n",
        "                 learning_rate=1e-3, epochs=10):\n",
        "        self._export_path = export_path\n",
        "        self._buffer_size = buffer_size\n",
        "        self._batch_size = batch_size\n",
        "        self._learning_rate = learning_rate\n",
        "        self._epochs = epochs\n",
        "    \n",
        "        self._build_model()\n",
        "        self.train_dataset, self.test_dataset = self._prepare_dataset()\n",
        "    \n",
        "    # функция для предварительной обрабоки изображения\n",
        "    def preprocess_fn(self, x):\n",
        "        \n",
        "        # УПРАЖНЕНИЕ: приведите x к tf.float32 с помощью функции tf.cast ().\n",
        "        # Вы также должны нормализовать значения x, чтобы они находились в диапазоне [0, 1].\n",
        "        x = tf.cast(x, tf.float32) / 255.0\n",
        "            \n",
        "        return x\n",
        "        \n",
        "    def _build_model(self):\n",
        "        \n",
        "        # УПРАЖНЕНИЕ: Постройте модель в соответствии с summary модели, показанной выше.\n",
        "        self._model = tf.keras.models.Sequential([\n",
        "            tf.keras.layers.Input(shape=(28, 28, 1), dtype=tf.uint8),\n",
        "            \n",
        "            # Используйте слой Lambda, чтобы использовать функцию self.preprocess_fn, \n",
        "            # определенную выше, для предварительной обработки изображений.\n",
        "            # ВАШ КОД ЗДЕСЬ\n",
        "            tf.keras.layers.Lambda(self.preprocess_fn),\n",
        "            # Создайте слой Conv2D с 8 фильтрами, размером ядра 3 и padding='same'.\n",
        "            # YOUR CODE HERE\n",
        "            tf.keras.layers.Conv2D(filters = 8, kernel_size=3, padding=\"same\"),\n",
        "            # Создайте слой MaxPool2D()со значениями по умалчанию\n",
        "            # YOUR CODE HERE\n",
        "            tf.keras.layers.MaxPool2D(),\n",
        "            # Создайте слой Conv2D  с 16 фильтрами, размером ядра 3 и padding='same'.\n",
        "            # YOUR CODE HERE\n",
        "            tf.keras.layers.Conv2D(filters = 16, kernel_size=3, padding=\"same\"),\n",
        "            # Создайте слой MaxPool2D()со значениями по умалчанию\n",
        "            # YOUR CODE HERE\n",
        "            tf.keras.layers.MaxPool2D(),\n",
        "            # Создайте слой Conv2D  с 32 фильтрами, размером ядра 3 и padding='same'.\n",
        "            # YOUR CODE HERE\n",
        "            tf.keras.layers.Conv2D(filters = 32, kernel_size=3, padding=\"same\"),\n",
        "            # Создайте слои Flatten и Dense как описано выше в model.summary()\n",
        "            # YOUR CODE HERE\n",
        "            tf.keras.layers.Flatten(),\n",
        "            tf.keras.layers.Dense(128),\n",
        "            tf.keras.layers.Dense(10, activation=\"softmax\")\n",
        "        ])\n",
        "        # УПРАЖНЕНИЕ: определите оптимизатор, функцию потерь и метрики.\n",
        "        # Используйте оптимизатор tf.keras.optimizers.Adam и установите\n",
        "        # скорость обучения для self._learning_rate.\n",
        "        optimizer_fn = tf.keras.optimizers.Adam(learning_rate=self._learning_rate)\n",
        "        \n",
        "        # Используйте sparse_categorical_crossentropy в качестве функции потерь.\n",
        "        loss_fn = \"sparse_categorical_crossentropy\"\n",
        "        \n",
        "        # установите метрику accuracy.\n",
        "        metrics_list = [\"accuracy\"]\n",
        "     \n",
        "        # Compile the model.\n",
        "        self._model.compile(optimizer_fn, loss=loss_fn, metrics=metrics_list)\n",
        "        \n",
        "    def _prepare_dataset(self):\n",
        "        # УПРАЖНЕНИЕ: загрузите набор данных MNIST с помощью tfds.load (). \n",
        "        # Вам следует загрузить изображения и также метки, им соответствующие;\n",
        "        # загружаем как тестовые, так и тренировочные.\n",
        "        dataset = tfds.load('mnist', split=['train', 'test'],  as_supervised=True)\n",
        "        \n",
        "        # УПРАЖНЕНИЕ: разделите ваш датасет на 'train' and 'test'\n",
        "        train_dataset, test_dataset = dataset\n",
        "        \n",
        "        return train_dataset, test_dataset\n",
        "    \n",
        "    def train(self):\n",
        "        # УПРАЖНЕНИЕ: перемешайте и разделите на пакеты набор self.train_dataset. \n",
        "        # Используйте self._buffer_size как буфер перемешивания \n",
        "        # и self._batch_size как размер пакета\n",
        "        dataset_tr = self.train_dataset.shuffle(self._buffer_size).batch(self._batch_size)\n",
        "        \n",
        "        # Тренируйте модель указанное количество эпох\n",
        "        self._model.fit(dataset_tr, epochs=self._epochs)\n",
        "        \n",
        "    def test(self):\n",
        "\n",
        "        # УПРАЖНЕНИЕ: разделите на пакеты self.test_dataset. Используйте размер пакета 32.\n",
        "        dataset_te = self.test_dataset.batch(32)\n",
        "        \n",
        "        # оценка датасета\n",
        "        results = self._model.evaluate(dataset_te)\n",
        "    \n",
        "        # вывод значений метрики оценки модели.\n",
        "        for name, value in zip(self._model.metrics_names, results):\n",
        "            print(\"%s: %.3f\" % (name, value))\n",
        "            \n",
        "    def export_model(self):\n",
        "        # Сохранение модели.\n",
        "        tf.saved_model.save(self._model, self._export_path)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dDAjgDe7lp4"
      },
      "source": [
        "## Тренировка, оценивание и сохранение  модели\n",
        "\n",
        "Теперь мы будем использовать созданный выше класс MNIST для создания объекта mnist. При создании нашего объекта `mnist` мы будем использовать словарь для передачи наших параметров обучения. Затем мы вызовем методы `train` и `export_model` для обучения и сохранения нашей модели. Наконец, мы вызовем метод `test`, чтобы оценить нашу модель после обучения.\n",
        "\n",
        "**ПРИМЕЧАНИЕ:** тренировка модели на 5 эпохах займет около 12 минут.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6Ba6ileois3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31db586f-254e-479c-b2e1-1e41c28e791a"
      },
      "source": [
        "# Определить параметры тренировки.\n",
        "args = {'export_path': './saved_model',\n",
        "        'buffer_size': 1000,\n",
        "        'batch_size': 32,\n",
        "        'learning_rate': 1e-3,\n",
        "        'epochs': 5\n",
        "}\n",
        "\n",
        "# Создание объекта mnist. \n",
        "mnist = MNIST(**args)\n",
        "\n",
        "# Тренировка модели.\n",
        "mnist.train()\n",
        "\n",
        "# Сохранение модели.\n",
        "mnist.export_model()\n",
        "\n",
        "# Оценивание тренированной модели MNIST.\n",
        "mnist.test()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset mnist/3.0.1 (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to /root/tensorflow_datasets/mnist/3.0.1...\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Dataset mnist is hosted on GCS. It will automatically be downloaded to your\n",
            "local data directory. If you'd instead prefer to read directly from our public\n",
            "GCS bucket (recommended if you're running on GCP), you can instead pass\n",
            "`try_gcs=True` to `tfds.load` or set `data_dir=gs://tfds-data/datasets`.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mDataset mnist downloaded and prepared to /root/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1527 - accuracy: 0.9532\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0800 - accuracy: 0.9761\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0691 - accuracy: 0.9785\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0612 - accuracy: 0.9814\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0549 - accuracy: 0.9828\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./saved_model/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./saved_model/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 4ms/step - loss: 0.0575 - accuracy: 0.9823\n",
            "loss: 0.057\n",
            "accuracy: 0.982\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sotJ7pQm7umV"
      },
      "source": [
        "## Создание файла tarball\n",
        "Метод `export_model` сохранил нашу модель в формате TensorFlow SavedModel в каталоге `./Saved_model`. Формат SavedModel сохраняет нашу модель и ее веса в различных файлах и каталогах. Так распространять модель будет довольно затруднительно. Поэтому будет удобно создать единый сжатый файл, содержащий все файлы и папки нашей модели. Для этого мы будем использовать программу архивирования tar, чтобы создать tarball (похоже на Zip-файл), содержащий нашу SavedModel.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTOmizWLZ5wf"
      },
      "source": [
        "# Создание a tarball из SavedModel.\n",
        "!tar -cz -f module.tar.gz -C ./saved_model ."
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuLDqA-RZ5wg"
      },
      "source": [
        "## Проверка Tarball\n",
        "Мы можем разархивировать наш tarball, чтобы убедиться, что он содерждит все файлы и каталоги нашей сохраненной модели"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NknIrjE1ovkF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "410e2bd9-9f6a-4238-eafa-7428456905e5"
      },
      "source": [
        "# Проверка tarball.\n",
        "!tar -tf module.tar.gz"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./\n",
            "./assets/\n",
            "./saved_model.pb\n",
            "./variables/\n",
            "./variables/variables.index\n",
            "./variables/variables.data-00000-of-00001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8LjCeO474N4"
      },
      "source": [
        "## Моделирование условий сервера\n",
        "\n",
        "После того, как мы проверили наш архив, мы можем моделировать состояние сервера. В обычном сценарии мы получаем наш модуль TF Hub с удаленного сервера, используя дескриптор модуля. Однако, поскольку этот нутбук не может размещать сервер, вместо этого мы укажем дескриптор модуля на каталог, в котором хранится наша SavedModel.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-8vmmtVxJVF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd512fee-4327-4810-ff38-4fe1378258c0"
      },
      "source": [
        "!rm -rf ./module\n",
        "!mkdir -p module\n",
        "!tar xvzf module.tar.gz -C ./module"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./\n",
            "./assets/\n",
            "./saved_model.pb\n",
            "./variables/\n",
            "./variables/variables.index\n",
            "./variables/variables.data-00000-of-00001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSmU1oZgxJZS"
      },
      "source": [
        "# Определим дескриптор модуля.\n",
        "MODULE_HANDLE = './module'"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgPZ5jQGZ5wh"
      },
      "source": [
        "## Load the TF Hub Module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2lOfoKab5Rv"
      },
      "source": [
        "# УПРАЖНЕНИЕ: загрузить наш хаб-модуль с использованием hub.load\n",
        "model = hub.load(MODULE_HANDLE)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjwQrBjVZ5wj"
      },
      "source": [
        "## Тестирование модуля TF Hub\n",
        "\n",
        "Теперь мы протестируем наш модуль TF Hub с изображениями из тестового набора данных MNIST."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCmeWVj_ovno"
      },
      "source": [
        "# УПРАЖНЕНИЕ: загрузить датасет MNIST, раздел 'test', с использованием tfds.load(). \n",
        "# Вам следует загрузить изображения вместе с их метками.\n",
        "\n",
        "dataset = tfds.load('mnist', split=tfds.Split.TEST, as_supervised=True)\n",
        "\n",
        "# УПРАЖНЕНИЕ: Разделить датасет на пакеты, используя batch size = 32.\n",
        "test_dataset = dataset.batch(32)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wY9bVLTayn3H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12eb75cf-aeeb-4b15-b976-423046d78d17"
      },
      "source": [
        "# Тестирование модуля TF Hub на одном пакете данных\n",
        "for batch_data in test_dataset.take(1):\n",
        "    outputs = model(batch_data[0])\n",
        "    outputs = np.argmax(outputs, axis=-1)\n",
        "    print('Predicted Labels:', outputs)\n",
        "    print('True Labels:     ', batch_data[1].numpy())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted Labels: [2 0 4 8 7 6 0 6 3 1 8 0 7 9 8 4 5 3 4 0 6 6 3 0 2 3 6 6 7 4 0 3]\n",
            "True Labels:      [2 0 4 8 7 6 0 6 3 1 8 0 7 9 8 4 5 3 4 0 6 6 3 0 2 3 6 6 7 4 9 3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vA89jXmNZ5wk"
      },
      "source": [
        "Мы видим, что модель правильно предсказывает метки для большинства изображений в пакете."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciRPFhPg8FWH"
      },
      "source": [
        "## Оценка модели с помощью Keras\n",
        "\n",
        "В ячейке ниже вы интегрируете модуль TensorFlow Hub в высокоуровневый Keras API."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMjnPFOjxmus"
      },
      "source": [
        "# УПРАЖНЕНИЕ: интегрировать ваш модуль TensorFlow Hub в модель keras.Sequential\n",
        "# Вам следует использовать hub.KerasLayer и  убедиться, что вы используете\n",
        "# правильные значения output_shape и input_shape. \n",
        "# Вы также должны использовать tf.uint8 для параметра dtype.\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    hub.KerasLayer(model, output_shape=[10], input_shape=[28,28,1], \n",
        "                           dtype=tf.uint8)\n",
        "]) \n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy', \n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShGHxh0Wx7lW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0605f84a-e0bc-4ba5-c2e4-85df3de77ba7"
      },
      "source": [
        "# Оценка модели на test_dataset.\n",
        "results = model.evaluate(test_dataset)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 4ms/step - loss: 0.0575 - accuracy: 0.9823\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZ6jUqbDx7s4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "272242e8-d61c-420c-955b-35f18e0cd05b"
      },
      "source": [
        "#Вывод значений метрики оценки\n",
        "for name, value in zip(model.metrics_names, results):\n",
        "    print(\"%s: %.3f\" % (name, value))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss: 0.057\n",
            "accuracy: 0.982\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}